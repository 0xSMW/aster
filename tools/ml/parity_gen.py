#!/usr/bin/env python3
"""
Generate a small self-contained Aster parity runner from a golden.json file.

This is harness tooling (NOT part of the Aster compiler/toolchain).
"""

from __future__ import annotations

import argparse
import json
import os
from typing import Any, Iterable


def repo_root() -> str:
    return os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))


def flatten(x: Any) -> list[float]:
    if x is None:
        return []
    if isinstance(x, (int, float)):
        return [float(x)]
    if isinstance(x, list):
        out: list[float] = []
        for v in x:
            out.extend(flatten(v))
        return out
    raise TypeError(f"unsupported json value in tensor: {type(x)}")


def f_lit(x: float) -> str:
    # Aster float literal grammar is simple (no exponent). Our generator values
    # are small and exactly representable, so repr() is stable and non-exponent.
    s = repr(float(x))
    if "e" in s or "E" in s:
        # Fallback: force fixed-point.
        s = f"{x:.12f}".rstrip("0").rstrip(".")
        if "." not in s:
            s += ".0"
    if "." not in s:
        s += ".0"
    return s


def emit(lines: list[str], s: str) -> None:
    lines.append(s)


def emit_if_ret1(lines: list[str], indent: str, cond_expr: str) -> None:
    emit(lines, f"{indent}if {cond_expr} then")
    emit(lines, f"{indent}    return 1")


def emit_ptr_fill(lines: list[str], indent: str, ptr_name: str, vals: Iterable[float]) -> None:
    for i, v in enumerate(vals):
        emit(lines, f"{indent}{ptr_name}[{i}] = {f_lit(v)}")


def emit_ptr_checks(
    lines: list[str],
    indent: str,
    case_name: str,
    label: str,
    ptr_name: str,
    want_vals: list[float],
    tol_name: str,
) -> None:
    for i, want in enumerate(want_vals):
        w = f_lit(want)
        emit(lines, f"{indent}if abs_f32({ptr_name}[{i}] - {w}) > {tol_name} then")
        emit(lines, f"{indent}    printf(\"FAIL {case_name} {label} idx %llu\\n\", {i})")
        emit(lines, f"{indent}    return 1")


def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--golden", type=str, required=True)
    ap.add_argument("--out", type=str, required=True)
    args = ap.parse_args()

    with open(args.golden, "r", encoding="utf-8") as f:
        g = json.load(f)

    cases: list[dict[str, Any]] = g.get("cases", [])
    out_lines: list[str] = []

    emit(out_lines, "# Generated by tools/ml/parity_gen.py. DO NOT EDIT.")
    emit(out_lines, "")
    emit(out_lines, "use core.libc")
    emit(out_lines, "use aster_ml.tensor")
    emit(out_lines, "use aster_ml.gradient")
    emit(out_lines, "")
    emit(out_lines, "def abs_f32(x is f32) returns f32")
    emit(out_lines, "    if x < 0.0 then")
    emit(out_lines, "        return 0.0 - x")
    emit(out_lines, "    return x")
    emit(out_lines, "")
    emit(out_lines, "def main() returns i32")
    emit(out_lines, "    var tol is f32 = 0.0001")

    for case in cases:
        name = str(case["name"])
        dtype = str(case.get("dtype", ""))
        if dtype != "float32":
            raise SystemExit(f"unsupported dtype in v0 parity runner: {dtype} ({name})")

        x_shape = [int(v) for v in case.get("x_shape", [])]
        y_shape = [int(v) for v in case.get("y_shape", [])]
        x_vals = flatten(case.get("x"))
        y_vals = flatten(case.get("y"))
        out_val = float(case.get("out"))
        xg_vals = flatten(case.get("x_grad"))
        yg_vals = flatten(case.get("y_grad"))

        emit(out_lines, "")
        emit(out_lines, f"    # case: {name}")
        emit(out_lines, f"    printf(\"case {name}\\n\")")

        if name.startswith("add_f32_"):
            r, c = x_shape
            assert y_shape == x_shape
            assert len(x_vals) == r * c and len(y_vals) == r * c
            assert len(xg_vals) == r * c and len(yg_vals) == r * c

            emit(out_lines, "    var a is Tensor")
            emit(out_lines, "    var b is Tensor")
            emit(out_lines, "    var tmp is Tensor")
            emit(out_lines, "    var s is Tensor")
            emit_if_ret1(out_lines, "    ", f"tensor_init_leaf(&a, 2, {r}, {c}, 1, 1) != 0")
            emit_if_ret1(out_lines, "    ", f"tensor_init_leaf(&b, 2, {r}, {c}, 1, 1) != 0")

            emit(out_lines, "    var ap is slice of f32 = a.data.data")
            emit_ptr_fill(out_lines, "    ", "ap", x_vals)
            emit(out_lines, "    var bp is slice of f32 = b.data.data")
            emit_ptr_fill(out_lines, "    ", "bp", y_vals)

            emit_if_ret1(out_lines, "    ", "tensor_add(&tmp, &a, &b) != 0")
            emit_if_ret1(out_lines, "    ", "tensor_sum(&s, &tmp) != 0")
            emit_if_ret1(out_lines, "    ", "tensor_backward(&s) != 0")

            emit(out_lines, "    var sp is slice of f32 = s.data.data")
            emit(out_lines, f"    if abs_f32(sp[0] - {f_lit(out_val)}) > tol then")
            emit(out_lines, "        printf(\"FAIL add out\\n\")")
            emit(out_lines, "        return 1")

            emit(out_lines, "    var gap is slice of f32 = a.grad.data")
            emit_ptr_checks(out_lines, "    ", name, "x_grad", "gap", xg_vals, "tol")
            emit(out_lines, "    var gbp is slice of f32 = b.grad.data")
            emit_ptr_checks(out_lines, "    ", name, "y_grad", "gbp", yg_vals, "tol")

            emit(out_lines, "    tensor_free(&s)")
            emit(out_lines, "    tensor_free(&tmp)")
            emit(out_lines, "    tensor_free(&b)")
            emit(out_lines, "    tensor_free(&a)")

        elif name.startswith("mul_f32_"):
            r, c = x_shape
            assert y_shape == x_shape
            assert len(x_vals) == r * c and len(y_vals) == r * c
            assert len(xg_vals) == r * c and len(yg_vals) == r * c

            emit(out_lines, "    var a is Tensor")
            emit(out_lines, "    var b is Tensor")
            emit(out_lines, "    var tmp is Tensor")
            emit(out_lines, "    var s is Tensor")
            emit_if_ret1(out_lines, "    ", f"tensor_init_leaf(&a, 2, {r}, {c}, 1, 1) != 0")
            emit_if_ret1(out_lines, "    ", f"tensor_init_leaf(&b, 2, {r}, {c}, 1, 1) != 0")

            emit(out_lines, "    var ap is slice of f32 = a.data.data")
            emit_ptr_fill(out_lines, "    ", "ap", x_vals)
            emit(out_lines, "    var bp is slice of f32 = b.data.data")
            emit_ptr_fill(out_lines, "    ", "bp", y_vals)

            emit_if_ret1(out_lines, "    ", "tensor_mul(&tmp, &a, &b) != 0")
            emit_if_ret1(out_lines, "    ", "tensor_sum(&s, &tmp) != 0")
            emit_if_ret1(out_lines, "    ", "tensor_backward(&s) != 0")

            emit(out_lines, "    var sp is slice of f32 = s.data.data")
            emit(out_lines, f"    if abs_f32(sp[0] - {f_lit(out_val)}) > tol then")
            emit(out_lines, "        printf(\"FAIL mul out\\n\")")
            emit(out_lines, "        return 1")

            emit(out_lines, "    var gap is slice of f32 = a.grad.data")
            emit_ptr_checks(out_lines, "    ", name, "x_grad", "gap", xg_vals, "tol")
            emit(out_lines, "    var gbp is slice of f32 = b.grad.data")
            emit_ptr_checks(out_lines, "    ", name, "y_grad", "gbp", yg_vals, "tol")

            emit(out_lines, "    tensor_free(&s)")
            emit(out_lines, "    tensor_free(&tmp)")
            emit(out_lines, "    tensor_free(&b)")
            emit(out_lines, "    tensor_free(&a)")

        elif name.startswith("matmul_f32_"):
            m, k = x_shape
            k2, n = y_shape
            assert k2 == k
            assert len(x_vals) == m * k and len(y_vals) == k * n
            assert len(xg_vals) == m * k and len(yg_vals) == k * n

            emit(out_lines, "    var a is Tensor")
            emit(out_lines, "    var b is Tensor")
            emit(out_lines, "    var tmp is Tensor")
            emit(out_lines, "    var s is Tensor")
            emit_if_ret1(out_lines, "    ", f"tensor_init_leaf(&a, 2, {m}, {k}, 1, 1) != 0")
            emit_if_ret1(out_lines, "    ", f"tensor_init_leaf(&b, 2, {k}, {n}, 1, 1) != 0")

            emit(out_lines, "    var ap is slice of f32 = a.data.data")
            emit_ptr_fill(out_lines, "    ", "ap", x_vals)
            emit(out_lines, "    var bp is slice of f32 = b.data.data")
            emit_ptr_fill(out_lines, "    ", "bp", y_vals)

            emit_if_ret1(out_lines, "    ", "tensor_matmul(&tmp, &a, &b) != 0")
            emit_if_ret1(out_lines, "    ", "tensor_sum(&s, &tmp) != 0")
            emit_if_ret1(out_lines, "    ", "tensor_backward(&s) != 0")

            emit(out_lines, "    var sp is slice of f32 = s.data.data")
            emit(out_lines, f"    if abs_f32(sp[0] - {f_lit(out_val)}) > tol then")
            emit(out_lines, "        printf(\"FAIL matmul out\\n\")")
            emit(out_lines, "        return 1")

            emit(out_lines, "    var gap is slice of f32 = a.grad.data")
            emit_ptr_checks(out_lines, "    ", name, "x_grad", "gap", xg_vals, "tol")
            emit(out_lines, "    var gbp is slice of f32 = b.grad.data")
            emit_ptr_checks(out_lines, "    ", name, "y_grad", "gbp", yg_vals, "tol")

            emit(out_lines, "    tensor_free(&s)")
            emit(out_lines, "    tensor_free(&tmp)")
            emit(out_lines, "    tensor_free(&b)")
            emit(out_lines, "    tensor_free(&a)")

        elif name.startswith("relu_f32_"):
            r, c = x_shape
            assert y_shape == []
            assert len(x_vals) == r * c
            assert len(xg_vals) == r * c

            emit(out_lines, "    var a is Tensor")
            emit(out_lines, "    var tmp is Tensor")
            emit(out_lines, "    var s is Tensor")
            emit_if_ret1(out_lines, "    ", f"tensor_init_leaf(&a, 2, {r}, {c}, 1, 1) != 0")

            emit(out_lines, "    var ap is slice of f32 = a.data.data")
            emit_ptr_fill(out_lines, "    ", "ap", x_vals)

            emit_if_ret1(out_lines, "    ", "tensor_relu(&tmp, &a) != 0")
            emit_if_ret1(out_lines, "    ", "tensor_sum(&s, &tmp) != 0")
            emit_if_ret1(out_lines, "    ", "tensor_backward(&s) != 0")

            emit(out_lines, "    var sp is slice of f32 = s.data.data")
            emit(out_lines, f"    if abs_f32(sp[0] - {f_lit(out_val)}) > tol then")
            emit(out_lines, "        printf(\"FAIL relu out\\n\")")
            emit(out_lines, "        return 1")

            emit(out_lines, "    var gap is slice of f32 = a.grad.data")
            emit_ptr_checks(out_lines, "    ", name, "x_grad", "gap", xg_vals, "tol")

            emit(out_lines, "    tensor_free(&s)")
            emit(out_lines, "    tensor_free(&tmp)")
            emit(out_lines, "    tensor_free(&a)")

        elif name.startswith("permute_f32_"):
            d0, d1, d2 = x_shape
            assert y_shape == []
            assert len(x_vals) == d0 * d1 * d2
            assert len(xg_vals) == d0 * d1 * d2

            emit(out_lines, "    var a is Tensor")
            emit(out_lines, "    var tmp is Tensor")
            emit(out_lines, "    var s is Tensor")
            emit_if_ret1(out_lines, "    ", f"tensor_init_leaf(&a, 3, {d0}, {d1}, {d2}, 1) != 0")

            emit(out_lines, "    var ap is slice of f32 = a.data.data")
            emit_ptr_fill(out_lines, "    ", "ap", x_vals)

            emit_if_ret1(out_lines, "    ", "tensor_permute_3(&tmp, &a, 2, 0, 1) != 0")
            emit_if_ret1(out_lines, "    ", "tensor_sum(&s, &tmp) != 0")
            emit_if_ret1(out_lines, "    ", "tensor_backward(&s) != 0")

            emit(out_lines, "    var sp is slice of f32 = s.data.data")
            emit(out_lines, f"    if abs_f32(sp[0] - {f_lit(out_val)}) > tol then")
            emit(out_lines, "        printf(\"FAIL permute out\\n\")")
            emit(out_lines, "        return 1")

            emit(out_lines, "    var gap is slice of f32 = a.grad.data")
            emit_ptr_checks(out_lines, "    ", name, "x_grad", "gap", xg_vals, "tol")

            emit(out_lines, "    tensor_free(&s)")
            emit(out_lines, "    tensor_free(&tmp)")
            emit(out_lines, "    tensor_free(&a)")

        else:
            raise SystemExit(f"unknown case name: {name}")

    emit(out_lines, "")
    emit(out_lines, "    printf(\"ok\\n\")")
    emit(out_lines, "    return 0")

    os.makedirs(os.path.dirname(args.out), exist_ok=True)
    with open(args.out, "w", encoding="utf-8") as f:
        f.write("\n".join(out_lines))
        f.write("\n")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
