#!/usr/bin/env python3
"""
Generate a small self-contained Aster parity runner from a golden.json file.

This is harness tooling (NOT part of the Aster compiler/toolchain).
"""

from __future__ import annotations

import argparse
import json
import os
from typing import Any, Iterable


def flatten(x: Any) -> list[float]:
    if x is None:
        return []
    if isinstance(x, (int, float)):
        return [float(x)]
    if isinstance(x, list):
        out: list[float] = []
        for v in x:
            out.extend(flatten(v))
        return out
    raise TypeError(f"unsupported json value in tensor: {type(x)}")


def f_lit(x: float) -> str:
    # Aster float literal grammar is simple (no exponent).
    s = repr(float(x))
    if "e" in s or "E" in s:
        s = f"{x:.12f}".rstrip("0").rstrip(".")
        if "." not in s:
            s += ".0"
    if "." not in s:
        s += ".0"
    return s


def emit(lines: list[str], s: str) -> None:
    lines.append(s)


def emit_if_ret1(lines: list[str], indent: str, cond_expr: str) -> None:
    emit(lines, f"{indent}if {cond_expr} then")
    emit(lines, f"{indent}    return 1")


def emit_ptr_fill(lines: list[str], indent: str, ptr_name: str, vals: Iterable[float]) -> None:
    for i, v in enumerate(vals):
        emit(lines, f"{indent}{ptr_name}[{i}] = {f_lit(v)}")


def emit_ptr_checks(
    lines: list[str],
    indent: str,
    case_name: str,
    label: str,
    ptr_name: str,
    want_vals: list[float],
    tol_name: str,
) -> None:
    for i, want in enumerate(want_vals):
        w = f_lit(want)
        emit(lines, f"{indent}if abs_f32({ptr_name}[{i}] - {w}) > {tol_name} then")
        emit(lines, f'{indent}    printf("FAIL {case_name} {label} idx %llu\\n", {i})')
        emit(lines, f"{indent}    return 1")


def emit_alloc_dims(lines: list[str], indent: str, name: str, dims: list[int]) -> None:
    nd = len(dims)
    emit(lines, f"{indent}var {name} is slice of usize = malloc({nd} * 8)")
    emit_if_ret1(lines, indent, f"{name} is null")
    for i, d in enumerate(dims):
        emit(lines, f"{indent}{name}[{i}] = {d}")


def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--golden", type=str, required=True)
    ap.add_argument("--out", type=str, required=True)
    args = ap.parse_args()

    with open(args.golden, "r", encoding="utf-8") as f:
        g = json.load(f)

    cases: list[dict[str, Any]] = g.get("cases", [])
    out_lines: list[str] = []

    emit(out_lines, "# Generated by tools/ml/parity_gen.py. DO NOT EDIT.")
    emit(out_lines, "")
    emit(out_lines, "use core.libc")
    emit(out_lines, "use aster_ml.tensor")
    emit(out_lines, "use aster_ml.gradient")
    emit(out_lines, "")
    emit(out_lines, "def abs_f32(x is f32) returns f32")
    emit(out_lines, "    if x < 0.0 then")
    emit(out_lines, "        return 0.0 - x")
    emit(out_lines, "    return x")
    emit(out_lines, "")
    emit(out_lines, "def main() returns i32")
    emit(out_lines, "    var tol is f32 = 0.0001")

    for case in cases:
        name = str(case["name"])
        dtype = str(case.get("dtype", ""))
        if dtype != "float32":
            raise SystemExit(f"unsupported dtype in v1 parity runner: {dtype} ({name})")

        x_shape = [int(v) for v in case.get("x_shape", [])]
        y_shape = [int(v) for v in case.get("y_shape", [])]
        x_vals = flatten(case.get("x"))
        y_vals = flatten(case.get("y"))
        out_val = float(case.get("out"))
        xg_vals = flatten(case.get("x_grad"))
        yg_vals = flatten(case.get("y_grad"))

        emit(out_lines, "")
        emit(out_lines, f"    # case: {name}")
        emit(out_lines, f'    printf("case {name}\\n")')

        if name.startswith("add_f32_"):
            r, c = x_shape
            assert y_shape == x_shape
            assert len(x_vals) == r * c and len(y_vals) == r * c
            assert len(xg_vals) == r * c and len(yg_vals) == r * c

            emit(out_lines, "    var a is GradTensor")
            emit(out_lines, "    var b is GradTensor")
            emit(out_lines, "    var tmp is GradTensor")
            emit(out_lines, "    var s is GradTensor")
            emit_alloc_dims(out_lines, "    ", "dims", [r, c])
            emit_if_ret1(out_lines, "    ", "grad_tensor_init_leaf(&a, 2, dims, 1) != 0")
            emit_if_ret1(out_lines, "    ", "grad_tensor_init_leaf(&b, 2, dims, 1) != 0")
            emit(out_lines, "    free(dims)")

            emit(out_lines, "    var ap is slice of f32 = tensor_data_ptr(&a.data)")
            emit_ptr_fill(out_lines, "    ", "ap", x_vals)
            emit(out_lines, "    var bp is slice of f32 = tensor_data_ptr(&b.data)")
            emit_ptr_fill(out_lines, "    ", "bp", y_vals)

            emit_if_ret1(out_lines, "    ", "grad_tensor_add(&tmp, &a, &b) != 0")
            emit_if_ret1(out_lines, "    ", "grad_tensor_sum_all(&s, &tmp) != 0")
            emit_if_ret1(out_lines, "    ", "grad_tensor_backward(&s) != 0")

            emit(out_lines, "    var sp is slice of f32 = tensor_data_ptr(&s.data)")
            emit(out_lines, f"    if abs_f32(sp[0] - {f_lit(out_val)}) > tol then")
            emit(out_lines, '        printf("FAIL add out\\n")')
            emit(out_lines, "        return 1")

            emit(out_lines, "    var gap is slice of f32 = tensor_data_ptr(&a.grad)")
            emit_ptr_checks(out_lines, "    ", name, "x_grad", "gap", xg_vals, "tol")
            emit(out_lines, "    var gbp is slice of f32 = tensor_data_ptr(&b.grad)")
            emit_ptr_checks(out_lines, "    ", name, "y_grad", "gbp", yg_vals, "tol")

            emit(out_lines, "    grad_tensor_free(&s)")
            emit(out_lines, "    grad_tensor_free(&tmp)")
            emit(out_lines, "    grad_tensor_free(&b)")
            emit(out_lines, "    grad_tensor_free(&a)")

        elif name.startswith("mul_f32_"):
            r, c = x_shape
            assert y_shape == x_shape
            assert len(x_vals) == r * c and len(y_vals) == r * c
            assert len(xg_vals) == r * c and len(yg_vals) == r * c

            emit(out_lines, "    var a is GradTensor")
            emit(out_lines, "    var b is GradTensor")
            emit(out_lines, "    var tmp is GradTensor")
            emit(out_lines, "    var s is GradTensor")
            emit_alloc_dims(out_lines, "    ", "dims", [r, c])
            emit_if_ret1(out_lines, "    ", "grad_tensor_init_leaf(&a, 2, dims, 1) != 0")
            emit_if_ret1(out_lines, "    ", "grad_tensor_init_leaf(&b, 2, dims, 1) != 0")
            emit(out_lines, "    free(dims)")

            emit(out_lines, "    var ap is slice of f32 = tensor_data_ptr(&a.data)")
            emit_ptr_fill(out_lines, "    ", "ap", x_vals)
            emit(out_lines, "    var bp is slice of f32 = tensor_data_ptr(&b.data)")
            emit_ptr_fill(out_lines, "    ", "bp", y_vals)

            emit_if_ret1(out_lines, "    ", "grad_tensor_mul(&tmp, &a, &b) != 0")
            emit_if_ret1(out_lines, "    ", "grad_tensor_sum_all(&s, &tmp) != 0")
            emit_if_ret1(out_lines, "    ", "grad_tensor_backward(&s) != 0")

            emit(out_lines, "    var sp is slice of f32 = tensor_data_ptr(&s.data)")
            emit(out_lines, f"    if abs_f32(sp[0] - {f_lit(out_val)}) > tol then")
            emit(out_lines, '        printf("FAIL mul out\\n")')
            emit(out_lines, "        return 1")

            emit(out_lines, "    var gap is slice of f32 = tensor_data_ptr(&a.grad)")
            emit_ptr_checks(out_lines, "    ", name, "x_grad", "gap", xg_vals, "tol")
            emit(out_lines, "    var gbp is slice of f32 = tensor_data_ptr(&b.grad)")
            emit_ptr_checks(out_lines, "    ", name, "y_grad", "gbp", yg_vals, "tol")

            emit(out_lines, "    grad_tensor_free(&s)")
            emit(out_lines, "    grad_tensor_free(&tmp)")
            emit(out_lines, "    grad_tensor_free(&b)")
            emit(out_lines, "    grad_tensor_free(&a)")

        elif name.startswith("matmul_f32_"):
            m, k = x_shape
            k2, n = y_shape
            assert k2 == k
            assert len(x_vals) == m * k and len(y_vals) == k * n
            assert len(xg_vals) == m * k and len(yg_vals) == k * n

            emit(out_lines, "    var a is GradTensor")
            emit(out_lines, "    var b is GradTensor")
            emit(out_lines, "    var tmp is GradTensor")
            emit(out_lines, "    var s is GradTensor")

            emit_alloc_dims(out_lines, "    ", "dims_a", [m, k])
            emit_if_ret1(out_lines, "    ", "grad_tensor_init_leaf(&a, 2, dims_a, 1) != 0")
            emit(out_lines, "    free(dims_a)")

            emit_alloc_dims(out_lines, "    ", "dims_b", [k, n])
            emit_if_ret1(out_lines, "    ", "grad_tensor_init_leaf(&b, 2, dims_b, 1) != 0")
            emit(out_lines, "    free(dims_b)")

            emit(out_lines, "    var ap is slice of f32 = tensor_data_ptr(&a.data)")
            emit_ptr_fill(out_lines, "    ", "ap", x_vals)
            emit(out_lines, "    var bp is slice of f32 = tensor_data_ptr(&b.data)")
            emit_ptr_fill(out_lines, "    ", "bp", y_vals)

            emit_if_ret1(out_lines, "    ", "grad_tensor_matmul(&tmp, &a, &b) != 0")
            emit_if_ret1(out_lines, "    ", "grad_tensor_sum_all(&s, &tmp) != 0")
            emit_if_ret1(out_lines, "    ", "grad_tensor_backward(&s) != 0")

            emit(out_lines, "    var sp is slice of f32 = tensor_data_ptr(&s.data)")
            emit(out_lines, f"    if abs_f32(sp[0] - {f_lit(out_val)}) > tol then")
            emit(out_lines, '        printf("FAIL matmul out\\n")')
            emit(out_lines, "        return 1")

            emit(out_lines, "    var gap is slice of f32 = tensor_data_ptr(&a.grad)")
            emit_ptr_checks(out_lines, "    ", name, "x_grad", "gap", xg_vals, "tol")
            emit(out_lines, "    var gbp is slice of f32 = tensor_data_ptr(&b.grad)")
            emit_ptr_checks(out_lines, "    ", name, "y_grad", "gbp", yg_vals, "tol")

            emit(out_lines, "    grad_tensor_free(&s)")
            emit(out_lines, "    grad_tensor_free(&tmp)")
            emit(out_lines, "    grad_tensor_free(&b)")
            emit(out_lines, "    grad_tensor_free(&a)")

        elif name.startswith("relu_f32_"):
            r, c = x_shape
            assert y_shape == []
            assert len(x_vals) == r * c
            assert len(xg_vals) == r * c

            emit(out_lines, "    var a is GradTensor")
            emit(out_lines, "    var tmp is GradTensor")
            emit(out_lines, "    var s is GradTensor")
            emit_alloc_dims(out_lines, "    ", "dims", [r, c])
            emit_if_ret1(out_lines, "    ", "grad_tensor_init_leaf(&a, 2, dims, 1) != 0")
            emit(out_lines, "    free(dims)")

            emit(out_lines, "    var ap is slice of f32 = tensor_data_ptr(&a.data)")
            emit_ptr_fill(out_lines, "    ", "ap", x_vals)

            emit_if_ret1(out_lines, "    ", "grad_tensor_relu(&tmp, &a) != 0")
            emit_if_ret1(out_lines, "    ", "grad_tensor_sum_all(&s, &tmp) != 0")
            emit_if_ret1(out_lines, "    ", "grad_tensor_backward(&s) != 0")

            emit(out_lines, "    var sp is slice of f32 = tensor_data_ptr(&s.data)")
            emit(out_lines, f"    if abs_f32(sp[0] - {f_lit(out_val)}) > tol then")
            emit(out_lines, '        printf("FAIL relu out\\n")')
            emit(out_lines, "        return 1")

            emit(out_lines, "    var gap is slice of f32 = tensor_data_ptr(&a.grad)")
            emit_ptr_checks(out_lines, "    ", name, "x_grad", "gap", xg_vals, "tol")

            emit(out_lines, "    grad_tensor_free(&s)")
            emit(out_lines, "    grad_tensor_free(&tmp)")
            emit(out_lines, "    grad_tensor_free(&a)")

        elif name.startswith("permute_f32_"):
            d0, d1, d2 = x_shape
            assert y_shape == []
            assert len(x_vals) == d0 * d1 * d2
            assert len(xg_vals) == d0 * d1 * d2

            # Build leaf as 1D, then reshape + permute, to exercise movement autograd.
            emit(out_lines, "    var raw is GradTensor")
            emit(out_lines, "    var a is GradTensor")
            emit(out_lines, "    var tmp is GradTensor")
            emit(out_lines, "    var s is GradTensor")

            emit_alloc_dims(out_lines, "    ", "dims1", [d0 * d1 * d2])
            emit_if_ret1(out_lines, "    ", "grad_tensor_init_leaf(&raw, 1, dims1, 1) != 0")
            emit(out_lines, "    free(dims1)")

            emit(out_lines, "    var rp is slice of f32 = tensor_data_ptr(&raw.data)")
            emit_ptr_fill(out_lines, "    ", "rp", x_vals)

            emit_alloc_dims(out_lines, "    ", "dims3", [d0, d1, d2])
            emit_if_ret1(out_lines, "    ", "grad_tensor_reshape(&a, &raw, 3, dims3) != 0")
            emit(out_lines, "    free(dims3)")

            # tinygrad case uses permute(2,0,1)
            emit_alloc_dims(out_lines, "    ", "perm", [2, 0, 1])
            emit_if_ret1(out_lines, "    ", "grad_tensor_permute(&tmp, &a, perm) != 0")
            emit(out_lines, "    free(perm)")

            emit_if_ret1(out_lines, "    ", "grad_tensor_sum_all(&s, &tmp) != 0")
            emit_if_ret1(out_lines, "    ", "grad_tensor_backward(&s) != 0")

            emit(out_lines, "    var sp is slice of f32 = tensor_data_ptr(&s.data)")
            emit(out_lines, f"    if abs_f32(sp[0] - {f_lit(out_val)}) > tol then")
            emit(out_lines, '        printf("FAIL permute out\\n")')
            emit(out_lines, "        return 1")

            emit(out_lines, "    var gap is slice of f32 = tensor_data_ptr(&a.grad)")
            emit_ptr_checks(out_lines, "    ", name, "x_grad", "gap", xg_vals, "tol")

            emit(out_lines, "    grad_tensor_free(&s)")
            emit(out_lines, "    grad_tensor_free(&tmp)")
            emit(out_lines, "    grad_tensor_free(&a)")
            emit(out_lines, "    grad_tensor_free(&raw)")

        else:
            raise SystemExit(f"unknown case name: {name}")

    emit(out_lines, "")
    emit(out_lines, '    printf("ok\\n")')
    emit(out_lines, "    return 0")

    os.makedirs(os.path.dirname(args.out), exist_ok=True)
    with open(args.out, "w", encoding="utf-8") as f:
        f.write("\n".join(out_lines))
        f.write("\n")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

